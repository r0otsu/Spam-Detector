{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook is inspired by the article \"Detecting Spam in Emails\", by Towards Data Science\n",
    "Ramya Vidiyala. The link to the article is: https://towardsdatascience.com/spam-detection-in-emails-de0398ea3b48\n",
    "\n",
    "**Overview**\n",
    "\n",
    "Usually, classification problems can be split into 2 categories: binary classification (only 2 possible label) and multi-classification (more than 2 label class). In this notebook, my goal is to explore and understand the process of classifying email as spam or legitimate. This is a classic binary classification problem. There are real-world problem can be solved by this method such as by detecting unsocilicited and unwanted emails, we can't prevent harmful and spam message from anonymous users (such as in gmail). Therefore, we can protect the privacy and improving user experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T04:52:30.808094Z",
     "iopub.status.busy": "2023-12-15T04:52:30.807073Z",
     "iopub.status.idle": "2023-12-15T04:52:30.846233Z",
     "shell.execute_reply": "2023-12-15T04:52:30.845549Z",
     "shell.execute_reply.started": "2023-12-15T04:52:30.808054Z"
    }
   },
   "outputs": [],
   "source": [
    "#Import library\n",
    "#Data preprocessing\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ngrams\n",
    "import re\n",
    "\n",
    "#Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "#Feature Engineering\n",
    "import string\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#Machine Learning Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Embedding,Dropout,Activation,Bidirectional\n",
    "import tensorflow as tf\n",
    "\n",
    "#Evaluation Metric\n",
    "from sklearn.metrics import confusion_matrix,f1_score, precision_score,recall_score\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T04:52:30.847749Z",
     "iopub.status.busy": "2023-12-15T04:52:30.847459Z",
     "iopub.status.idle": "2023-12-15T04:52:32.694889Z",
     "shell.execute_reply": "2023-12-15T04:52:32.694250Z",
     "shell.execute_reply.started": "2023-12-15T04:52:30.847720Z"
    }
   },
   "outputs": [],
   "source": [
    "#Load the dataset\n",
    "df=pd.read_csv(\"/kaggle/input/email-spam-classification-dataset/combined_data.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset:\n",
    "* '1' indicates that the email is classified as spam.\n",
    "* '0' denotes that the email is legitimate (ham)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check for missing value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T04:52:32.695957Z",
     "iopub.status.busy": "2023-12-15T04:52:32.695722Z",
     "iopub.status.idle": "2023-12-15T04:52:32.705759Z",
     "shell.execute_reply": "2023-12-15T04:52:32.705130Z",
     "shell.execute_reply.started": "2023-12-15T04:52:32.695932Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the shape of the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T04:52:32.707508Z",
     "iopub.status.busy": "2023-12-15T04:52:32.707265Z",
     "iopub.status.idle": "2023-12-15T04:52:32.716848Z",
     "shell.execute_reply": "2023-12-15T04:52:32.716137Z",
     "shell.execute_reply.started": "2023-12-15T04:52:32.707483Z"
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert to lower case letter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T04:52:32.717904Z",
     "iopub.status.busy": "2023-12-15T04:52:32.717635Z",
     "iopub.status.idle": "2023-12-15T04:52:33.050923Z",
     "shell.execute_reply": "2023-12-15T04:52:33.050188Z",
     "shell.execute_reply.started": "2023-12-15T04:52:32.717878Z"
    }
   },
   "outputs": [],
   "source": [
    "df['text'] = df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove Numbers and Special Character**\n",
    "\n",
    "Often, characters, symbols, and numbers usually don't contribute to differentiating spam from legitimate emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T04:52:33.052140Z",
     "iopub.status.busy": "2023-12-15T04:52:33.051883Z",
     "iopub.status.idle": "2023-12-15T04:52:35.898257Z",
     "shell.execute_reply": "2023-12-15T04:52:35.897437Z",
     "shell.execute_reply.started": "2023-12-15T04:52:33.052113Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_special_characters(word):\n",
    "    return word.translate(str.maketrans('', '', string.punctuation))\n",
    "df['text'] = df['text'].apply(remove_special_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove English stop-words**\n",
    "\n",
    "Stopwords e like ‘the’, ‘a’,.... can be removed from the text because they don’t provide valuable information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T04:52:35.899696Z",
     "iopub.status.busy": "2023-12-15T04:52:35.899404Z",
     "iopub.status.idle": "2023-12-15T04:54:02.668531Z",
     "shell.execute_reply": "2023-12-15T04:54:02.667650Z",
     "shell.execute_reply.started": "2023-12-15T04:52:35.899658Z"
    }
   },
   "outputs": [],
   "source": [
    "#Define stop-words in English\n",
    "ENGLISH_STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stop_words(words):\n",
    "    return [word for word in words if word not in ENGLISH_STOP_WORDS]\n",
    "\n",
    "#Tokenize the text\n",
    "df['text'] = df['text'].apply(word_tokenize)\n",
    "\n",
    "#Remove stop words\n",
    "df['text'] = df['text'].apply(remove_stop_words)\n",
    "\n",
    "#Rejoin words for EDA\n",
    "df['text'] = df['text'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removal of hyperlinks**\n",
    "\n",
    "Some emails contains URL which doesn't provide any valuable information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T04:54:02.669989Z",
     "iopub.status.busy": "2023-12-15T04:54:02.669736Z",
     "iopub.status.idle": "2023-12-15T04:54:02.834071Z",
     "shell.execute_reply": "2023-12-15T04:54:02.833177Z",
     "shell.execute_reply.started": "2023-12-15T04:54:02.669963Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_hyperlink(word):\n",
    "    return re.sub(r\"http\\S+\", \"\", word)\n",
    "\n",
    "df['text'] = df['text'].apply(remove_hyperlink)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "**Let check with an example of a spam text**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T04:54:02.835261Z",
     "iopub.status.busy": "2023-12-15T04:54:02.835010Z",
     "iopub.status.idle": "2023-12-15T04:54:02.845209Z",
     "shell.execute_reply": "2023-12-15T04:54:02.844552Z",
     "shell.execute_reply.started": "2023-12-15T04:54:02.835236Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range (1,3):\n",
    "    print(\"Email #\"+str(i))\n",
    "    spam_email = df[df['label'] == 1]['text'].iloc[i]\n",
    "    print(spam_email+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why those emails are classified as spam?**\n",
    "\n",
    "Email 1: Medication Offers\n",
    "* Unsolicited Advertising: The email is promoting medications like Viagra, Levitra, Cialis, and others, which is a common theme in spam emails.\n",
    "* Random Words and Phrases: The message includes a mix of random words and pharmaceutical names, a tactic often used in spam to bypass filters.\n",
    "* Suspicious Links: The mention of a website to visit for purchasing medications is a red flag. Legitimate pharmaceutical companies typically don't market their products in such a manner.\n",
    "* Irregular Formatting: Use of random characters and inconsistent spacing (e.g., \"wulvob\", \"qnb ikud\") is common in spam to evade detection algorithms.\n",
    "* Lack of Personalization: The email is generic and does not address any recipient directly, implying it was sent to a mass audience.\n",
    "\n",
    "Email 2: Offering University Degrees\n",
    "* False Promises: Offering a university degree with no need for tests, classes, books, or exams is unrealistic and typical of educational scam emails.\n",
    "* Anonymity: The assurance of confidentiality and the lack of any legitimate university names.\n",
    "* 24/7 Availability: Claiming availability \"24 hours a day, 7 days a week\" is not typical for legitimate educational institutions.\n",
    "* Lack of Specific Details: The email is vague about the nature of the degrees and the process, which is characteristic of spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Therefore, the main characteristics of spam emails are often:**\n",
    "\n",
    "*Unsolicited Advertising:* Spam emails often promote products or services without the recipient's prior consent.\n",
    "\n",
    "*Random and Irregular Text:* They frequently contain a mix of random words, phrases, and characters to evade spam filters.\n",
    "\n",
    "*Deceptive Offers:* Spam messages commonly include unrealistic or too-good-to-be-true offers, like quick degrees or cheap medications.\n",
    "\n",
    "*Lack of Personalization:* These emails are typically generic, lacking direct addressing or personalization, indicating mass distribution.\n",
    "\n",
    "*Suspicious Links or Instructions:* They often contain suspicious links or unclear instructions, potentially leading to phishing sites or scams.\n",
    "\n",
    "*Incoherent Content:* The content in spam emails is often disjointed and lacks a clear, coherent message or purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let check with an example of a legitimate text**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T04:54:02.848867Z",
     "iopub.status.busy": "2023-12-15T04:54:02.848356Z",
     "iopub.status.idle": "2023-12-15T04:54:02.858554Z",
     "shell.execute_reply": "2023-12-15T04:54:02.857844Z",
     "shell.execute_reply.started": "2023-12-15T04:54:02.848836Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range (1,3):\n",
    "    print(\"Email #\"+str(i))\n",
    "    legitimate_email = df[df['label'] == 0]['text'].iloc[i]\n",
    "    print(legitimate_email+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why those emails are classified as legitimate?**\n",
    "\n",
    "Email 1: Technical Discussion\n",
    "* Specific and Relevant Content: The email contains detailed, technical information about using rsync, a legitimate software tool, indicating a specific, purposeful discussion.\n",
    "* Personalized and Contextual: The email is part of a conversation, with references to previous messages and specific individuals, showing it's targeted and relevant to the recipients.\n",
    "* Professional Tone: The language and structure are professional and focused on a specific technical topic, which is typical in legitimate correspondence.\n",
    "* No Unsolicited Offers or Links: There are no out-of-place promotions or suspicious links, which are common in spam.\n",
    "\n",
    "Email 2: Legitimate Promotion\n",
    "* Official Communication: This email is from CNN, a recognized organization, promoting a scheduled show, which is a normal practice for media companies.\n",
    "* Clear, Relevant Information: It provides detailed information about a specific event (interview with Michael Moore) and how to engage with it, indicating its legitimacy.\n",
    "* Opt-in Confirmation: It mentions that the recipient has agreed to receive such emails, suggesting it's part of a legitimate subscription.\n",
    "* Proper Unsubscribe Option: The presence of a clear unsubscribe option is a sign of legitimate marketing practices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Therefore, the main characteristics of legitimate emails are often:**\n",
    "\n",
    "*Relevant and Specific Content:* Legitimate emails typically contain detailed, specific information related to the sender and recipient's shared context or interests.\n",
    "\n",
    "*Personalized Communication:* They often address the recipient directly or reference previous interactions, indicating a personalized and targeted approach.\n",
    "\n",
    "*Professional and Coherent Tone:* Legitimate emails usually maintain a professional, coherent language and tone, appropriate for the subject matter.\n",
    "\n",
    "*Affiliation with Recognized Entities:* They are often associated with known organizations or entities, providing a layer of authenticity.\n",
    "\n",
    "*Opt-in and Unsubscribe Options:* Legitimate commercial or promotional emails usually include clear options for opting in and unsubscribing, respecting recipient preferences.\n",
    "\n",
    "*Absence of Suspicious Links or Offers:* Unlike spam, they typically do not contain unsolicited offers, misleading links, or requests for sensitive information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spam email vs. Legitimate email chart**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T04:54:02.860043Z",
     "iopub.status.busy": "2023-12-15T04:54:02.859525Z",
     "iopub.status.idle": "2023-12-15T04:54:03.035020Z",
     "shell.execute_reply": "2023-12-15T04:54:03.033576Z",
     "shell.execute_reply.started": "2023-12-15T04:54:02.860011Z"
    }
   },
   "outputs": [],
   "source": [
    "# Count the number of spam and legitimate emails\n",
    "email_counts = df['label'].value_counts()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pie(email_counts, labels=['Spam (1)', 'Legitimate (0)'], autopct='%1.1f%%', startangle=140, colors=['tomato', 'lightblue'])\n",
    "plt.title('Comparison of Spam and Legitimate Emails')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart shows that 52.6% of the emails are classified as spam, while 47.4% are classified as legitimate. This nearly even split indicates that the dataset is fairly balanced between the two classes, which is good for training a machine learning model because it reduces the risk of the model being biased towards one class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Frequency Analysis**\n",
    "\n",
    "Create a bar chart of the most frequent words in both spam and legitimate emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T04:54:03.037417Z",
     "iopub.status.busy": "2023-12-15T04:54:03.036826Z",
     "iopub.status.idle": "2023-12-15T04:54:06.053977Z",
     "shell.execute_reply": "2023-12-15T04:54:06.053270Z",
     "shell.execute_reply.started": "2023-12-15T04:54:03.037340Z"
    }
   },
   "outputs": [],
   "source": [
    "# Separate the spam and legitimate emails\n",
    "spam_emails = df[df['label'] == 1]['text']\n",
    "legit_emails = df[df['label'] == 0]['text']\n",
    "\n",
    "# Count word frequencies for spam\n",
    "spam_words = Counter()\n",
    "spam_emails.apply(lambda x: spam_words.update(x.split()))\n",
    "\n",
    "# Count word frequencies for legitimate emails\n",
    "legit_words = Counter()\n",
    "legit_emails.apply(lambda x: legit_words.update(x.split()))\n",
    "\n",
    "# Get the most common words in spam and legitimate emails\n",
    "spam_common = spam_words.most_common(10)\n",
    "legit_common = legit_words.most_common(10)\n",
    "\n",
    "# Convert to DataFrame\n",
    "spam_common_df = pd.DataFrame(spam_common, columns=['Word', 'Frequency'])\n",
    "legit_common_df = pd.DataFrame(legit_common, columns=['Word', 'Frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T04:54:06.055191Z",
     "iopub.status.busy": "2023-12-15T04:54:06.054944Z",
     "iopub.status.idle": "2023-12-15T04:54:06.432368Z",
     "shell.execute_reply": "2023-12-15T04:54:06.431694Z",
     "shell.execute_reply.started": "2023-12-15T04:54:06.055165Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting the most common words in spam emails\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(spam_common_df['Word'], spam_common_df['Frequency'], color='red')\n",
    "plt.title('Most Common Words in Spam Emails')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Plotting the most common words in legitimate emails\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(legit_common_df['Word'], legit_common_df['Frequency'], color='green')\n",
    "plt.title('Most Common Words in Legitimate Emails')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Cloud**\n",
    "\n",
    "Word clouds for spam and non-spam emails help visualize the most common words in each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T04:54:06.433577Z",
     "iopub.status.busy": "2023-12-15T04:54:06.433316Z",
     "iopub.status.idle": "2023-12-15T04:54:34.522794Z",
     "shell.execute_reply": "2023-12-15T04:54:34.522112Z",
     "shell.execute_reply.started": "2023-12-15T04:54:06.433551Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate a word cloud image for spam words\n",
    "spam_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(spam_emails))\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(spam_wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**N-gram Analysis**\n",
    "\n",
    "The purpose of n-gram analysis is to explore the most common sequences of two or three words in spam and legitimate emails. This could help in understanding common phrases used in both types of emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T04:54:34.523942Z",
     "iopub.status.busy": "2023-12-15T04:54:34.523706Z",
     "iopub.status.idle": "2023-12-15T04:54:43.626276Z",
     "shell.execute_reply": "2023-12-15T04:54:43.625523Z",
     "shell.execute_reply.started": "2023-12-15T04:54:34.523916Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract n-grams from text\n",
    "def generate_ngrams(text, n=2):\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    # Ensure that there is no enough words\n",
    "    if len(words) >= n:\n",
    "        return [' '.join(grams) for grams in ngrams(words, n)]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Convert bigrams\n",
    "df['bigrams'] = df['text'].apply(lambda x: generate_ngrams(x, n=2))\n",
    "\n",
    "# count the frequencies\n",
    "bigram_counts = Counter([bigram for sublist in df['bigrams'] for bigram in sublist])\n",
    "\n",
    "# Get the most common bigrams\n",
    "most_common_bigrams = bigram_counts.most_common(10)\n",
    "\n",
    "# Convert to DataFrame for visualization\n",
    "bigrams_df = pd.DataFrame(most_common_bigrams, columns=['Bigram', 'Frequency'])\n",
    "\n",
    "# Plot the most common bigrams\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(bigrams_df['Bigram'], bigrams_df['Frequency'], color='skyblue')\n",
    "plt.xlabel('Frequency')\n",
    "plt.title('Top 10 Most Common Bigrams')\n",
    "plt.gca().invert_yaxis()  # Display the highest count at the top\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spliting data into train and test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T04:54:43.627412Z",
     "iopub.status.busy": "2023-12-15T04:54:43.627145Z",
     "iopub.status.idle": "2023-12-15T04:54:43.640799Z",
     "shell.execute_reply": "2023-12-15T04:54:43.640070Z",
     "shell.execute_reply.started": "2023-12-15T04:54:43.627385Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the data into features and target\n",
    "X = df['text']  \n",
    "y = df['label']  \n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization**\n",
    "\n",
    "Tokenization is the process of splitting text into smaller chunks, called tokens. Each token is an input to the machine learning algorithm as a feature. \n",
    "\n",
    "In Python, keras.preprocessing.text.Tokenizer is a utility function that help tokenizes a text into tokens while keeping only the words that frequently occur. After tokenizing the text, we often end up with a massive dictionary of words which won’t all be necessary. One solution for this problem is setting ‘max_features’ to select the top frequent words that we want to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T04:54:43.642041Z",
     "iopub.status.busy": "2023-12-15T04:54:43.641742Z",
     "iopub.status.idle": "2023-12-15T04:55:00.613126Z",
     "shell.execute_reply": "2023-12-15T04:55:00.612418Z",
     "shell.execute_reply.started": "2023-12-15T04:54:43.642010Z"
    }
   },
   "outputs": [],
   "source": [
    "max_features = 5000 \n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert texts to sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T04:55:00.614357Z",
     "iopub.status.busy": "2023-12-15T04:55:00.614096Z",
     "iopub.status.idle": "2023-12-15T04:55:00.617716Z",
     "shell.execute_reply": "2023-12-15T04:55:00.616955Z",
     "shell.execute_reply.started": "2023-12-15T04:55:00.614331Z"
    }
   },
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding**\n",
    "\n",
    "Padding is the step of making all tokens for all emails to equal size. Since we send input in batches of data points, information might be lost when inputs are of different sizes. So, making them the same size help eases batch updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T04:55:00.618731Z",
     "iopub.status.busy": "2023-12-15T04:55:00.618514Z",
     "iopub.status.idle": "2023-12-15T04:55:00.663076Z",
     "shell.execute_reply": "2023-12-15T04:55:00.662451Z",
     "shell.execute_reply.started": "2023-12-15T04:55:00.618708Z"
    }
   },
   "outputs": [],
   "source": [
    "#Average Length\n",
    "average_length = df['text'].apply(len).mean()\n",
    "print(\"Average Length: \"+str(average_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T04:55:00.664067Z",
     "iopub.status.busy": "2023-12-15T04:55:00.663841Z",
     "iopub.status.idle": "2023-12-15T04:55:01.247297Z",
     "shell.execute_reply": "2023-12-15T04:55:01.246582Z",
     "shell.execute_reply.started": "2023-12-15T04:55:00.664043Z"
    }
   },
   "outputs": [],
   "source": [
    "max_length = 500 #Set to average length\n",
    "\n",
    "# Padding sequences\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Label the encoding target variable**\n",
    "\n",
    "The model expect the target variable as a number instead of string. Therefore, we can use a Label encoder to convert our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T04:55:01.248499Z",
     "iopub.status.busy": "2023-12-15T04:55:01.248251Z",
     "iopub.status.idle": "2023-12-15T04:55:01.254611Z",
     "shell.execute_reply": "2023-12-15T04:55:01.253960Z",
     "shell.execute_reply.started": "2023-12-15T04:55:01.248473Z"
    }
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "\n",
    "To simplify with an example, let say we analyze a movie, which consists of a sequence of scenes. When we watch a scene, we can't understand the movie at whole, but rather in connection with previous scenes. Text works in a similar way, a machine learning model has to understand the text by utilizing already-learned text (like a human neural network).\n",
    "\n",
    "For those reasons, Recurrent neural network (RNN) is a perfect fit. It has a repeating module that takes input from the previous stage and gives its output as input to the next stage. One limit of RNNs is that we can only retain information from the most recent stage. To learn long-term dependencies, our network needs memorization power. Therfore, Long Short Term Memory Networks (LSTMs) works perfectly to solve this problem.\n",
    "\n",
    "In short, LSTMs are a special case of RNNs, which have similar chain-like structure as RNNs, but with a different repeating module structure. In this model, we will use Bi-directional LSTM.  In a Bi-directional LSTM, the input sequence is fed in two ways: one from past to future and one from future to past. This can provide additional context to the network and result in a fuller understanding of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-12-15T04:55:01.255772Z",
     "iopub.status.busy": "2023-12-15T04:55:01.255508Z",
     "iopub.status.idle": "2023-12-15T04:55:05.088639Z",
     "shell.execute_reply": "2023-12-15T04:55:05.087826Z",
     "shell.execute_reply.started": "2023-12-15T04:55:01.255744Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_vector_length = 32\n",
    "model = tf.keras.Sequential()\n",
    "model.add(Embedding(max_features, embedding_vector_length, input_length=max_length))\n",
    "model.add(Bidirectional(tf.keras.layers.LSTM(64)))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T04:55:05.089874Z",
     "iopub.status.busy": "2023-12-15T04:55:05.089609Z",
     "iopub.status.idle": "2023-12-15T05:42:07.141639Z",
     "shell.execute_reply": "2023-12-15T05:42:07.140721Z",
     "shell.execute_reply.started": "2023-12-15T04:55:05.089847Z"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train_padded, y_train_encoded, \n",
    "                    batch_size=512, \n",
    "                    epochs=20, \n",
    "                    validation_data=(X_test_padded, y_test_encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary,  both training loss and accuracy show exceptional result, with the loss decreasing and accuracy over 99%, indicating effective learning on the training data. However, a key concern arises with the validation metrics such as the validation loss begins to increase after around the 5th epoch, and the validation accuracy starts to fluctuate and slightly decrease thereafter. This pattern, where the model performs exceptionally well on training data but less so on validation data, is a classic sign of overfitting. Further techniques are needed to address this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction and Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T05:42:07.143985Z",
     "iopub.status.busy": "2023-12-15T05:42:07.143259Z",
     "iopub.status.idle": "2023-12-15T05:42:53.889028Z",
     "shell.execute_reply": "2023-12-15T05:42:53.888007Z",
     "shell.execute_reply.started": "2023-12-15T05:42:07.143931Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_padded)\n",
    "y_predict = [1 if o > 0.5 else 0 for o in y_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision, Recall, F1 Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T05:42:53.890350Z",
     "iopub.status.busy": "2023-12-15T05:42:53.890076Z",
     "iopub.status.idle": "2023-12-15T05:42:53.947289Z",
     "shell.execute_reply": "2023-12-15T05:42:53.946582Z",
     "shell.execute_reply.started": "2023-12-15T05:42:53.890321Z"
    }
   },
   "outputs": [],
   "source": [
    "cf_matrix =confusion_matrix(y_test_encoded,y_predict)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_encoded,y_predict).ravel()\n",
    "print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test_encoded, y_predict)))\n",
    "print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test_encoded, y_predict)))\n",
    "print(\"F1 Score: {:.2f}%\".format(100 * f1_score(y_test_encoded,y_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T05:42:53.948394Z",
     "iopub.status.busy": "2023-12-15T05:42:53.948145Z",
     "iopub.status.idle": "2023-12-15T05:42:54.145550Z",
     "shell.execute_reply": "2023-12-15T05:42:54.144681Z",
     "shell.execute_reply.started": "2023-12-15T05:42:53.948368Z"
    }
   },
   "outputs": [],
   "source": [
    "ax= plt.subplot()\n",
    "#annot=True to annotate cells\n",
    "sns.heatmap(cf_matrix, annot=True, ax = ax,cmap='Blues',fmt='');\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');\n",
    "ax.set_ylabel('True labels');\n",
    "ax.set_title('Confusion Matrix');\n",
    "ax.xaxis.set_ticklabels(['Not Spam', 'Spam']); ax.yaxis.set_ticklabels(['Not Spam', 'Spam']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is good model such as it has an F1 score of 96.68%. However, these result are only fit to this specific training dataset. When using this model to real-world data, we need to actively monitor the model's performance over time. We can also improve the model by responding to results and adding features to improve the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this notebook, I created a spam detection model by converting text data into tokens, adding feature, creating a Bi-LSTM model, and fitting the model with these vectors. Similarly, these concepts and techniques learned can be applied to other real-world example such as building chatbots, text summarization, language translation models, or detect LLM."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3962399,
     "sourceId": 6897944,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30615,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
